

lab 6--

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, binom, poisson, bernoulli

def get_user_data():
    # Get the frequency distribution input from the user
    data_input = input("Enter the data values separated by commas (e.g., 10, 20, 30): ")
    frequencies_input = input("Enter the corresponding frequencies separated by commas (e.g., 2, 3, 4): ")
    
    # Convert the inputs into lists of integers
    data = list(map(int, data_input.split(',')))
    frequencies = list(map(int, frequencies_input.split(',')))
    
    return data, frequencies

def plot_normal_distribution(data, frequencies):
    # Fit and plot Normal distribution
    mean = np.mean(data)
    std_dev = np.std(data)
    
    x = np.linspace(min(data), max(data), 100)
    pdf = norm.pdf(x, mean, std_dev)

    plt.plot(x, pdf, 'r-', lw=2, label='Normal Distribution')
    plt.title('Normal Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability Density')
    plt.show()

def plot_binomial_distribution(data, frequencies):
    # Fit and plot Binomial distribution (assuming n is max(data) and p is mean/len(data))
    n = max(data)
    p = np.mean(data) / n
    
    x = np.arange(0, n+1)
    pmf = binom.pmf(x, n, p)

    plt.bar(x, pmf, alpha=0.7, color='b', label='Binomial Distribution')
    plt.title('Binomial Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability')
    plt.show()

def plot_poisson_distribution(data, frequencies):
    # Fit and plot Poisson distribution (lambda is the mean of the data)
    lam = np.mean(data)
    
    x = np.arange(0, max(data)+1)
    pmf = poisson.pmf(x, lam)

    plt.bar(x, pmf, alpha=0.7, color='g', label='Poisson Distribution')
    plt.title('Poisson Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability')
    plt.show()

def plot_bernoulli_distribution(data, frequencies):
    # Assuming binary outcome for Bernoulli
    success_prob = np.mean(data) / max(data)
    
    x = [0, 1]
    pmf = bernoulli.pmf(x, success_prob)

    plt.bar(x, pmf, alpha=0.7, color='purple', label='Bernoulli Distribution')
    plt.title('Bernoulli Distribution')
    plt.xlabel('Value')
    plt.ylabel('Probability')
    plt.show()

def analyze_distributions(data, frequencies):
    print("Analyzing Normal Distribution:")
    plot_normal_distribution(data, frequencies)
    
    print("Analyzing Binomial Distribution:")
    plot_binomial_distribution(data, frequencies)
    
    print("Analyzing Poisson Distribution:")
    plot_poisson_distribution(data, frequencies)
    
    print("Analyzing Bernoulli Distribution:")
    plot_bernoulli_distribution(data, frequencies)

# Main program
data, frequencies = get_user_data()
analyze_distributions(data, frequencies)





lab 7--

# Import necessary libraries
import numpy as np
import pandas as pd
from scipy import stats

# Sample data for demonstration
# One-sample test: A group of exam scores
exam_scores = np.array([85, 87, 90, 78, 88, 95, 82, 79, 94, 91])

# Two-sample test: Scores of two different groups
group_A = np.array([85, 89, 88, 90, 93, 85, 84, 79, 90, 87])
group_B = np.array([82, 86, 85, 87, 92, 80, 81, 78, 89, 85])

# Paired-sample test: Before and after treatment scores of the same group
before_treatment = np.array([82, 84, 88, 78, 80, 85, 90, 79, 87, 83])
after_treatment = np.array([85, 87, 89, 81, 83, 88, 92, 82, 89, 86])

# Function to perform one-sample t-test
def one_sample_ttest(data, population_mean):
    t_stat, p_value = stats.ttest_1samp(data, population_mean)
    return t_stat, p_value

# Function to perform two-sample t-test (independent samples)
def two_sample_ttest(group1, group2):
    t_stat, p_value = stats.ttest_ind(group1, group2)
    return t_stat, p_value

# Function to perform paired-sample t-test
def paired_sample_ttest(before, after):
    t_stat, p_value = stats.ttest_rel(before, after)
    return t_stat, p_value

# Analyze results of the t-tests
def analyze_ttest_results(t_stat, p_value, alpha=0.05):
    print(f"T-statistic: {t_stat}")
    print(f"P-value: {p_value}")
    if p_value < alpha:
        print("Result: The null hypothesis is rejected (statistically significant difference).")
    else:
        print("Result: The null hypothesis cannot be rejected (no statistically significant difference).")

# One-sample t-test: Compare exam scores with a population mean (e.g., 85)
print("One-Sample T-Test:")
t_stat, p_value = one_sample_ttest(exam_scores, 85)
analyze_ttest_results(t_stat, p_value)
print()

# Two-sample t-test: Compare the means of two independent groups
print("Two-Sample T-Test:")
t_stat, p_value = two_sample_ttest(group_A, group_B)
analyze_ttest_results(t_stat, p_value)
print()

# Paired-sample t-test: Compare before and after treatment of the same group
print("Paired-Sample T-Test:")
t_stat, p_value = paired_sample_ttest(before_treatment, after_treatment)
analyze_ttest_results(t_stat, p_value)


lab 9---

# Import required libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Generate sample data (or load your dataset here)
np.random.seed(42)  # For reproducibility
x = np.random.rand(100) * 100  # Random values for x
y = 2.5 * x + np.random.normal(0, 25, 100)  # Linear relation with noise

# Convert data into a DataFrame
data = pd.DataFrame({'X': x, 'Y': y})

# Compute Correlation
pearson_corr = data.corr(method='pearson')  # Pearson Correlation
spearman_corr, _ = spearmanr(data['X'], data['Y'])  # Spearman Rank Correlation

# Linear Regression
X = data['X'].values.reshape(-1, 1)  # Reshape for sklearn
Y = data['Y'].values
model = LinearRegression()
model.fit(X, Y)
Y_pred = model.predict(X)
regression_coeff = model.coef_[0]  # Slope
regression_intercept = model.intercept_  # Intercept
mse = mean_squared_error(Y, Y_pred)

# Print statistical results
print("Pearson Correlation Coefficient Matrix:")
print(pearson_corr)
print("\nSpearman Rank Correlation Coefficient:", spearman_corr)
print("\nLinear Regression Equation: Y = {:.2f}X + {:.2f}".format(regression_coeff, regression_intercept))
print("Mean Squared Error (MSE):", mse)

# Plot X-Y scatter plot with regression line
plt.figure(figsize=(8, 6))
plt.scatter(data['X'], data['Y'], color='blue', label='Data Points')
plt.plot(data['X'], Y_pred, color='red', label='Regression Line')
plt.title('X-Y Scatter Plot with Regression Line')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()

# Plot heatmap of correlation matrix
plt.figure(figsize=(6, 5))
sns.heatmap(pearson_corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Heatmap of Correlation Matrix')
plt.show()
lab 10 --

Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer

from sklearn.decomposition import PCA

from sklearn.preprocessing import StandardScaler



# Load the Wisconsin Breast Cancer dataset

data = load_breast_cancer()

X = data.data  # Features

y = data.target  # Target variable (0 = malignant, 1 = benign)

feature_names = data.feature_names

target_names = data.target_names



# Standardize the data (important for PCA)

scaler = StandardScaler()

X_scaled = scaler.fit_transform(X)

# Apply PCA

pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization

X_pca = pca.fit_transform(X_scaled)
# Get explained variance ratio for each component

explained_variance_ratio = pca.explained_variance_ratio_
# Create a DataFrame for visualization

pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])

pca_df['Target'] = y
# Plot the PCA results

plt.figure(figsize=(8, 6))

sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='Target', palette='Set1', alpha=0.8)

plt.title('PCA of Wisconsin Breast Cancer Dataset')

plt.xlabel('Principal Component 1')

plt.ylabel('Principal Component 2')

plt.legend(target_names)
plt.grid()
plt.show()
# Plot explained variance ratio

plt.figure(figsize=(8, 5))

plt.bar(range(1, 3), explained_variance_ratio, tick_label=['PCA1', 'PCA2'], color='skyblue')

plt.title('Explained Variance Ratio of PCA Components')

plt.xlabel('Principal Components')

plt.ylabel('Variance Explained')

plt.show()

# Full PCA with all components for analysis

pca_full = PCA()

X_pca_full = pca_full.fit_transform(X_scaled)

cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)



# Plot cumulative explained variance

plt.figure(figsize=(8, 5))

plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')

plt.title('Cumulative Explained Variance')

plt.xlabel('Number of Principal Components')

plt.ylabel('Cumulative Variance Explained')
plt.grid()
plt.show()
# Print key insights

print("PCA Analysis of Wisconsin Breast Cancer Dataset")

print("-------------------------------------------------")

print(f"Explained Variance (PCA1): {explained_variance_ratio[0]:.4f}")

print(f"Explained Variance (PCA2): {explained_variance_ratio[1]:.4f}")

print("Cumulative Variance Explained by All Components:")

for i, cum_var in enumerate(cumulative_variance, start=1):

    print(f"  Component {i}: {cum_var:.4f}")




lab 11 --
#Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler
# Load the Iris dataset
data = load_iris()
X = data.data  # Features
y = data.target  # Target variable (0, 1, 2)
target_names = data.target_names  # Class names

# Standardize the data (LDA benefits from scaling)

scaler = StandardScaler()

X_scaled = scaler.fit_transform(X)
# Apply Linear Discriminant Analysis (LDA)
lda = LinearDiscriminantAnalysis(n_components=2)  # Reduce to 2 components for visualization
X_lda = lda.fit_transform(X_scaled, y)
# Create a DataFrame for LDA-transformed data

lda_df = pd.DataFrame(X_lda, columns=['LDA1', 'LDA2'])

lda_df['Target'] = y
# Plot the LDA results in 2D space

plt.figure(figsize=(8, 6))

sns.scatterplot(data=lda_df, x='LDA1', y='LDA2', hue='Target', palette='Set1', style='Target', s=100)

plt.title('LDA of Iris Dataset')

plt.xlabel('Linear Discriminant 1')

plt.ylabel('Linear Discriminant 2')

plt.legend(title='Class', labels=target_names)

plt.grid()
plt.show()
# Print key insights
print("Linear Discriminant Analysis (LDA) Results")
print("--------------------------------------------------")
print("Explained Variance Ratio by LDA Components:")
for i, ratio in enumerate(lda.explained_variance_ratio_, start=1):
    print(f"  LDA{i}: {ratio:.4f}")





lab == 12

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the Iris dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)  # Features
y = X['petal length (cm)']  # Let's predict 'petal length' as the dependent variable
X = X.drop(columns=['petal length (cm)'])  # Remove 'petal length' from independent variables

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply Multiple Linear Regression
model = LinearRegression()
model.fit(X_train, y_train)  # Train the model

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print model performance metrics
print("Multiple Linear Regression Results")
print("----------------------------------")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"R-squared (RÂ²): {r2:.4f}")
print("\nModel Coefficients:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"  {feature}: {coef:.4f}")
print(f"Intercept: {model.intercept_:.4f}")

# Visualize actual vs predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2, linestyle='--')
plt.title('Actual vs Predicted Values (Test Set)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.grid()
plt.show()

# Pairplot to explore relationships in the dataset
sns.pairplot(pd.DataFrame(data.data, columns=data.feature_names), diag_kind='kde')
plt.suptitle('Pairplot of Iris Dataset Features', y=1.02)
plt.show()





lab 1=
# Import necessary libraries
import pandas as pd
import numpy as np
# 1. Combining and Merging Datasets
# Create two sample DataFrames
sales_data_1 = pd.DataFrame({
    'OrderID': [1, 2, 3, 4],
    'Product': ['Laptop', 'Tablet', 'Smartphone', 'Headphones'],
    'Sales': [1200, 800, 1500, 300]
})

sales_data_2 = pd.DataFrame({
    'OrderID': [3, 4, 5, 6],
    'Product': ['Smartphone', 'Headphones', 'Smartwatch', 'Tablet'],
    'Sales': [1500, 300, 200, 900]
})
# Display the DataFrames
print("Sales Data 1:\n", sales_data_1)
print("\nSales Data 2:\n", sales_data_2)
# Merge DataFrames based on 'OrderID' using an inner join
merged_data = pd.merge(sales_data_1, sales_data_2, on='OrderID', how='inner', suffixes=('_left', '_right'))
print("\nMerged Data (Inner Join):\n", merged_data)
# Concatenate the DataFrames vertically
combined_data = pd.concat([sales_data_1, sales_data_2], ignore_index=True)
print("\nCombined Data (Concatenated Vertically):\n", combined_data)
# 2. Reshaping Data with Melt
# Create a sample DataFrame for reshaping
reshaping_data = pd.DataFrame({
    'Month': ['Jan', 'Feb', 'Mar'],
    'Product_A': [100, 150, 130],
    'Product_B': [90, 80, 120]
})
print("\nReshaping Data (Original):\n", reshaping_data)
# Melt the DataFrame to reshape it from wide to long format
melted_data = pd.melt(reshaping_data, id_vars=['Month'], var_name='Product', value_name='Sales')
print("\nMelted Data (Long Format):\n", melted_data)
# 3. Pivoting Data
# Create a sample DataFrame for pivoting
pivot_data = pd.DataFrame({
    'Month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar'],
    'Product': ['Product_A', 'Product_B', 'Product_A', 'Product_B', 'Product_A', 'Product_B'],
    'Sales': [100, 90, 150, 80, 130, 120]
})
print("\nPivot Data (Original):\n", pivot_data)
# Pivot the DataFrame to reshape it back to wide format
pivoted_data = pivot_data.pivot(index='Month', columns='Product', values='Sales')
print("\nPivoted Data (Wide Format):\n", pivoted_data)
# 4. Handling Missing Data
# Introduce some missing values
pivoted_data.loc['Feb', 'Product_A'] = np.nan
pivoted_data.loc['Mar', 'Product_B'] = np.nan
print("\nPivoted Data with Missing Values:\n", pivoted_data)
# Fill missing values with the mean of each column
filled_data = pivoted_data.fillna(pivoted_data.mean())
print("\nFilled Data (Missing Values Handled):\n", filled_data)
# 5. Summary Statistics
print("\nSummary Statistics of Filled Data:\n", filled_data.describe())

OUTPUT:
Sales Data 1:
    OrderID     Product  Sales
0        1      Laptop   1200
1        2      Tablet    800
2        3  Smartphone   1500
3        4  Headphones    300

Sales Data 2:
    OrderID     Product  Sales
0        3  Smartphone   1500
1        4  Headphones    300
2        5  Smartwatch    200
3        6      Tablet    900

Merged Data (Inner Join):
    OrderID Product_left  Sales_left Product_right  Sales_right
0        3   Smartphone        1500    Smartphone         1500
1        4   Headphones         300    Headphones          300

Combined Data (Concatenated Vertically):
    OrderID     Product  Sales
0        1      Laptop   1200
1        2      Tablet    800
2        3  Smartphone   1500
3        4  Headphones    300
4        3  Smartphone   1500
5        4  Headphones    300
6        5  Smartwatch    200
7        6      Tablet    900

Reshaping Data (Original):
   Month  Product_A  Product_B
0   Jan        100         90
1   Feb        150         80
2   Mar        130        120

Melted Data (Long Format):
   Month    Product  Sales
0   Jan  Product_A    100
1   Feb  Product_A    150
2   Mar  Product_A    130
3   Jan  Product_B     90
4   Feb  Product_B     80
5   Mar  Product_B    120

Pivot Data (Original):
   Month    Product  Sales
0   Jan  Product_A    100
1   Jan  Product_B     90
2   Feb  Product_A    150
3   Feb  Product_B     80
4   Mar  Product_A    130
5   Mar  Product_B    120

Pivoted Data (Wide Format):
 Product  Product_A  Product_B
Month                        
Feb            150         80
Jan            100         90
Mar            130        120

Pivoted Data with Missing Values:
 Product  Product_A  Product_B
Month                        
Feb            NaN       80.0
Jan          100.0       90.0
Mar          130.0        NaN

Filled Data (Missing Values Handled):
 Product  Product_A  Product_B
Month                        
Feb          115.0       80.0
Jan          100.0       90.0
Mar          130.0       85.0

Summary Statistics of Filled Data:
 Product  Product_A  Product_B
count          3.0        3.0
mean         115.0       85.0
std           15.0        5.0
min          100.0       80.0
25%          107.5       82.5
50%          115.0       85.0
75%          122.5       87.5
max          130.0       90.0

lab 2=
String Manupulation:
# Sample text to work with
text = "  Hello, World! Welcome to Python programming.  "
# 1. Strip leading and trailing spaces
clean_text = text.strip()
print(f"Original Text: '{text}'")
print(f"Text after stripping spaces: '{clean_text}'")
# 2. Convert the text to uppercase
upper_text = clean_text.upper()
print(f"\nText in uppercase: '{upper_text}'")
# 3. Convert the text to lowercase
lower_text = clean_text.lower()
print(f"\nText in lowercase: '{lower_text}'")
# 4. Count occurrences of a substring (e.g., "o")
count_o = clean_text.count("o")
print(f"\nNumber of occurrences of 'o': {count_o}")
# 5. Replace a word in the string
replaced_text = clean_text.replace("Python", "Data Science")
print(f"\nText after replacing 'Python' with 'Data Science': '{replaced_text}'")
# 6. Find the position of a word in the string
position_world = clean_text.find("World")
print(f"\nPosition of 'World' in the text: {position_world}")
# 7. Split the text into words (by default on spaces)
words = clean_text.split()
print(f"\nList of words in the text: {words}")
# 8. Join the words back into a single string
joined_text = " ".join(words)
print(f"\nText after joining words: '{joined_text}'")
# 9. Check if the text starts with "Hello"
starts_with_hello = clean_text.startswith("Hello")
print(f"\nDoes the text start with 'Hello'? {starts_with_hello}")
# 10. Check if the text ends with a specific word (e.g., "programming.")
ends_with_programming = clean_text.endswith("programming.")
print(f"\nDoes the text end with 'programming.'? {ends_with_programming}")

OUTPUT:
Original Text: '  Hello, World! Welcome to Python programming.  '
Text after stripping spaces: 'Hello, World! Welcome to Python programming.'
Text in uppercase: 'HELLO, WORLD! WELCOME TO PYTHON PROGRAMMING.'
Text in lowercase: 'hello, world! welcome to python programming.'
Number of occurrences of 'o': 6
Text after replacing 'Python' with 'Data Science': 'Hello, World! Welcome to Data Science programming.'
Position of 'World' in the text: 7
List of words in the text: ['Hello,', 'World!', 'Welcome', 'to', 'Python', 'programming.']
Text after joining words: 'Hello, World! Welcome to Python programming.'
Does the text start with 'Hello'? True
Does the text end with 'programming.'? True

Regular Expressions:
import re
# Sample text
text = """
John's email is [email protected]. He said, "Python is awesome!!"    It's a     great   language.
Another email: [email protected]. 
"""
# 1. Remove special characters except for spaces and email-related characters.
# Using regex to remove non-alphabetic characters and non-email symbols
clean_text = re.sub(r"[^a-zA-Z0-9@\.\s]", "", text)
print("Text after removing special characters:")
print(clean_text)
# 2. Convert the text to lowercase
clean_text = clean_text.lower()
print("\nText after converting to lowercase:")
print(clean_text)
# 3. Replace multiple spaces with a single space
clean_text = re.sub(r"\s+", " ", clean_text)
print("\nText after replacing multiple spaces:")
print(clean_text)
# 4. Extract all words starting with a vowel (a, e, i, o, u)
vowel_words = re.findall(r"\b[aeiouAEIOU]\w+", clean_text)
print("\nWords starting with a vowel:")
print(vowel_words)
# 5. Replace email addresses with '[email protected]'
masked_text = re.sub(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "[email protected]", clean_text)
print("\nText after replacing emails:")
print(masked_text)

OUTPUT:
Text after removing special characters:
Johns email is email protected. He said Python is awesome    Its a     great   language.
Another email email protected. 

Text after converting to lowercase:
johns email is email protected. he said python is awesome   its  a     great   language.
another email email protected. 

Text after replacing multiple spaces:
 johns email is email protected. he said python is awesome its a great language. another email email protected. 

Words starting with a vowel:
['email', 'is', 'email', 'is', 'awesome', 'its', 'another', 'email', 'email']

Text after replacing emails:
 johns email is email protected. he said python is awesome its a great language. another email email protected. 

lab 3=
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import ExponentialSmoothing
# Create sample time series data
np.random.seed(42)
date_range = pd.date_range(start="2022-01-01", end="2023-01-01", freq="D")
data = pd.DataFrame({
    "Date": date_range,
    "Value_A": np.random.normal(100, 10, len(date_range)),
    "Value_B": np.random.normal(200, 20, len(date_range)),
})
# Set Date as the index
data.set_index("Date", inplace=True)
# GroupBy Mechanics
def groupby_mechanics(data):
    print("\n--- GroupBy Mechanics ---")
    # Group data by month and calculate mean
    grouped = data.resample('ME').mean()
    print(grouped)
    return grouped
# Data Formats: Vector and Multivariate
def data_formats(data):
    print("\n--- Data Formats ---")
    # Display data as vector
    print("\nVector Format:")
    print(data["Value_A"].head())
    # Display multivariate time series
    print("\nMultivariate Time Series:")
    print(data.head())
# Forecasting Example
def time_series_forecasting(data):
    print("\n--- Forecasting ---")
    # Select a single column for forecasting
    ts = data["Value_A"]
    # Train-Test Split
    train = ts[:int(0.8 * len(ts))]
    test = ts[int(0.8 * len(ts)):]
    # Fit the Holt-Winters Exponential Smoothing model
    model = ExponentialSmoothing(train, seasonal="add", seasonal_periods=30).fit()
    # Forecast for the test period
    forecast = model.forecast(len(test))
    # Plot results
    plt.figure(figsize=(12, 6))
    plt.plot(train, label="Train")
    plt.plot(test, label="Test")
    plt.plot(forecast, label="Forecast")
    plt.legend()
    plt.title("Time Series Forecasting")
    plt.show()
# Main function
if __name__ == "__main__":
    print("--- Time Series Data ---")
    print(data.head())
    # Grouping Mechanics
    monthly_data = groupby_mechanics(data)
    # Data Formats
    data_formats(data)
    # Time Series Forecasting
    time_series_forecasting(data)

OUTPUT:
--- Time Series Data ---
               Value_A     Value_B
Date                              
2022-01-01  104.967142  204.481850
2022-01-02   98.617357  200.251848
2022-01-03  106.476885  201.953522
2022-01-04  115.230299  184.539804
2022-01-05   97.658466  200.490203

--- GroupBy Mechanics ---
               Value_A     Value_B
Date                              
2022-01-31   97.985125  202.137470
2022-02-28   98.568317  204.960833
2022-03-31  100.439383  194.956405
2022-04-30   99.797484  198.429574
2022-05-31   99.161855  199.020262
2022-06-30  102.912924  192.752508
2022-07-31  100.983406  199.844253
2022-08-31   99.784632  201.134556
2022-09-30   99.089296  203.720687
2022-10-31  100.649960  198.150774
2022-11-30  102.325711  199.427682
2022-12-31   99.467543  197.195680
2023-01-31   95.987795  180.432544

--- Data Formats ---

Vector Format:
Date
2022-01-01    104.967142
2022-01-02     98.617357
2022-01-03    106.476885
2022-01-04    115.230299
2022-01-05     97.658466
Name: Value_A, dtype: float64

Multivariate Time Series:
               Value_A     Value_B
Date                              
2022-01-01  104.967142  204.481850
2022-01-02   98.617357  200.251848
2022-01-03  106.476885  201.953522
2022-01-04  115.230299  184.539804
2022-01-05   97.658466  200.490203

lab 4=
# Import necessary libraries
import numpy as np
import pandas as pd
def calculate_statistics(data, frequencies):
    # Create a DataFrame for the frequency distribution
    df = pd.DataFrame({'Value': data, 'Frequency': frequencies})
    # Calculate the total number of observations
    total = df['Frequency'].sum()
    # Calculate mean
    df['Weighted_Value'] = df['Value'] * df['Frequency']
    mean = df['Weighted_Value'].sum() / total
    # Calculate median
    cumulative_frequency = df['Frequency'].cumsum()
    median_index = cumulative_frequency.searchsorted(total / 2)
    median = df['Value'][median_index]
    # Calculate mode
    mode = df['Value'][df['Frequency'].idxmax()]
    # Calculate variance and standard deviation
    variance = np.average((df['Value'] - mean) ** 2, weights=df['Frequency'])
    std_deviation = np.sqrt(variance)
    # Calculate mean deviation
    mean_deviation = np.average(np.abs(df['Value'] - mean), weights=df['Frequency'])
    # Calculate quartile deviation
    q1 = np.percentile(data, 25)
    q3 = np.percentile(data, 75)
    quartile_deviation = (q3 - q1) / 2
    return {
        'Mean': mean,
        'Median': median,
        'Mode': mode,
        'Variance': variance,
        'Standard Deviation': std_deviation,
        'Mean Deviation': mean_deviation,
        'Quartile Deviation': quartile_deviation
    }
# Get user input for data and frequencies
data_input = input("Enter the data values separated by commas (e.g., 10, 20, 30): ")
frequencies_input = input("Enter the corresponding frequencies separated by commas (e.g., 1, 2, 3): ")
# Convert input strings to lists of integers
data = list(map(int, data_input.split(',')))
frequencies = list(map(int, frequencies_input.split(',')))
# Calculate statistics
statistics = calculate_statistics(data, frequencies)
# Display the results
for stat, value in statistics.items():
    print(f"{stat}: {value:.2f}")

OUTPUT:
Enter the data values separated by commas (e.g., 10, 20, 30):  20, 40, 60
Enter the corresponding frequencies separated by commas (e.g., 1, 2, 3):  7, 8, 9
Mean: 41.67
Median: 40.00
Mode: 60.00
Variance: 263.89
Standard Deviation: 16.24
Mean Deviation: 13.75
Quartile Deviation: 10.00

lab 5=
import numpy as np
from sklearn.model_selection import train_test_split, KFold, LeaveOneOut
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

X, y = np.array([[1], [2], [3], [4], [5]]), np.array([1.2, 2.3, 2.9, 4.1, 5.1])
model = LinearRegression()

def cross_validate(method):  # General cross-validation
    errors = [[], [], []]
    for train_idx, test_idx in method.split(X):
        model.fit(X[train_idx], y[train_idx])
        y_pred = model.predict(X[test_idx])
        errors[0].append(mean_squared_error(y[test_idx], y_pred, squared=False))
        errors[1].append(mean_absolute_error(y[test_idx], y_pred))
        errors[2].append(r2_score(y[test_idx], y_pred))
    return f"RMSE: {np.mean(errors[0]):.2f}, MAE: {np.mean(errors[1]):.2f}, R2: {np.mean(errors[2]):.2f}"

print("Validation Set ->", cross_validate(LeaveOneOut() if len(X) == 5 else KFold(2)))

lab 6=
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, binom, poisson, bernoulli

# Given data values and frequencies
data_values = [10, 30, 50, 70]
frequencies = [1, 2, 3, 4]

# Plot Normal Distribution
plt.subplot(2, 2, 1)
plt.hist(np.random.normal(0, 1, 1000), bins=30, density=True, alpha=0.6, color='g')
x = np.linspace(-4, 4, 100)
plt.plot(x, norm.pdf(x), 'k', linewidth=2)
plt.title('Normal Distribution')

# Plot Binomial Distribution
plt.subplot(2, 2, 2)
plt.hist(binom.rvs(10, 0.5, 1000), bins=10, density=True, alpha=0.6, color='b')
plt.title('Binomial Distribution')

# Plot Poisson Distribution
plt.subplot(2, 2, 3)
plt.hist(poisson.rvs(3, 1000), bins=15, density=True, alpha=0.6, color='r')
plt.title('Poisson Distribution')

# Plot Bernoulli Distribution
plt.subplot(2, 2, 4)
plt.hist(bernoulli.rvs(0.5, 1000), bins=2, density=True, alpha=0.6, color='y')
plt.title('Bernoulli Distribution')

# Plot custom frequency distribution
plt.figure(figsize=(6, 4))
plt.bar(data_values, frequencies, color='c', width=8)
plt.title('Custom Frequency Distribution')
plt.xlabel('Data Values')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

lab 7=
import numpy as np
from scipy import stats

# Sample Data
sample1 = np.array([25, 30, 35, 40, 45])
sample2 = np.array([28, 33, 38, 42, 46])

# One-sample t-test (test if the mean is 30)
t_stat_one, p_val_one = stats.ttest_1samp(sample1, 30)

# Two-sample t-test (compare means of two independent samples)
t_stat_two, p_val_two = stats.ttest_ind(sample1, sample2)

# Paired-sample t-test (compare paired data)
t_stat_paired, p_val_paired = stats.ttest_rel(sample1, sample2)

# Output Results
print(f"One-sample t-test: t-statistic = {t_stat_one}, p-value = {p_val_one}")
print(f"Two-sample t-test: t-statistic = {t_stat_two}, p-value = {p_val_two}")
print(f"Paired-sample t-test: t-statistic = {t_stat_paired}, p-value = {p_val_paired}")

lab 8=
import numpy as np
import pandas as pd
from scipy import stats

# Sample Data for One-way ANOVA
group1 = np.array([25, 30, 35, 40])
group2 = np.array([28, 33, 38, 42])
group3 = np.array([27, 32, 37, 41])

# One-way ANOVA (comparing means of three groups)
f_stat_oneway, p_val_oneway = stats.f_oneway(group1, group2, group3)

# Sample Data for Two-way ANOVA
data = {
    'Factor1': ['A', 'A', 'A', 'B', 'B', 'B'],
    'Factor2': ['X', 'Y', 'X', 'Y', 'X', 'Y'],
    'Values': [25, 30, 35, 40, 45, 50]
}
df = pd.DataFrame(data)

# Two-way ANOVA
anova_table = pd.pivot_table(df, values='Values', index='Factor1', columns='Factor2', aggfunc='mean')
f_stat_twoway, p_val_twoway = stats.f_oneway(*[df[df['Factor1'] == f]['Values'] for f in df['Factor1'].unique()])

# Output Results
print(f"One-way ANOVA: F-statistic = {f_stat_oneway}, p-value = {p_val_oneway}")
print(f"Two-way ANOVA: F-statistic = {f_stat_twoway}, p-value = {p_val_twoway}")



lab 9=
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Data
x = np.random.rand(50) * 10
y = 2.5 * x + np.random.normal(0, 5, 50)
data = pd.DataFrame({'X': x, 'Y': y})

# Correlation and Regression
corr = data.corr()
m, b = np.polyfit(x, y, 1)

# Output
print("Correlation:\n", corr)
print(f"Regression: Y = {m:.2f}*X + {b:.2f}")

# Plot
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.scatter(x, y)
plt.plot(x, m * x + b, color='red')
plt.show()

lab 10=
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler

# Load and scale the data
X, y = load_breast_cancer(return_X_y=True)
X_scaled = StandardScaler().fit_transform(X)

# Apply PCA
X_pca = PCA(n_components=2).fit_transform(X_scaled)

# Plot the results
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA - Wisconsin Breast Cancer Dataset')
plt.colorbar(label='Cancer Type (0: Malignant, 1: Benign)')
plt.show()

lab 11 = 
from sklearn.datasets import load_iris
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import matplotlib.pyplot as plt

# Load the Iris dataset
X, y = load_iris(return_X_y=True)

# Apply LDA
X_lda = LDA(n_components=2).fit_transform(X, y)

# Visualize the results
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', edgecolor='k', alpha=0.7)
plt.xlabel('LD1')
plt.ylabel('LD2')
plt.title('LDA - Iris Dataset')
plt.colorbar(label='Classes')
plt.show()


lab 12 = 
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset and prepare data
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression().fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred))
print("R2:", r2_score(y_test, y_pred))

# Visualize
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Predicted vs Actual')
plt.show()
